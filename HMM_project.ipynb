{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3db6f1-8514-4aef-9e02-dbfcfa300f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "245e820f-3d32-48ab-a1a6-3fa5fe7da997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "from matplotlib import cm, pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd14ae0-f223-43ff-a5c6-e4125dbf4d26",
   "metadata": {},
   "source": [
    "USING HIDDEN MARKOV MODEL FOR PART-OF SPEECH TAGGING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f65d33cb-deea-4802-8e21-be28fdd3ed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/hrantbaloyan/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/hrantbaloyan/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sentence: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n",
      "Predicted Tags: ['DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN']\n",
      "Accuracy: 88.73%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK data (if you don't have it already)\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# 1. Data Preparation (using NLTK's Penn Treebank)\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_data = tagged_sentences[:int(len(tagged_sentences) * 0.8)]\n",
    "test_data = tagged_sentences[int(len(tagged_sentences) * 0.8):]\n",
    "\n",
    "# Create vocabulary and tag set\n",
    "word_set = set()\n",
    "tag_set = set()\n",
    "for sent in train_data:\n",
    "    for word, tag in sent:\n",
    "        word_set.add(word.lower())\n",
    "        tag_set.add(tag)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(word_set)}\n",
    "tag_to_idx = {tag: i for i, tag in enumerate(tag_set)}\n",
    "idx_to_tag = {i: tag for i, tag in enumerate(tag_set)}\n",
    "\n",
    "# 2. Parameter Estimation (Training)\n",
    "\n",
    "# Initialize transition, emission, and initial probability matrices\n",
    "transition_probs = np.zeros((len(tag_set), len(tag_set)))\n",
    "emission_probs = np.zeros((len(tag_set), len(word_set)))\n",
    "initial_probs = np.zeros(len(tag_set))\n",
    "\n",
    "# Count occurrences for transition probabilities\n",
    "for sent in train_data:\n",
    "    for i in range(len(sent) - 1):\n",
    "        current_tag_idx = tag_to_idx[sent[i][1]]\n",
    "        next_tag_idx = tag_to_idx[sent[i+1][1]]\n",
    "        transition_probs[current_tag_idx, next_tag_idx] += 1\n",
    "\n",
    "# Count occurrences for emission probabilities\n",
    "for sent in train_data:\n",
    "    for word, tag in sent:\n",
    "        word_idx = word_to_idx[word.lower()]\n",
    "        tag_idx = tag_to_idx[tag]\n",
    "        emission_probs[tag_idx, word_idx] += 1\n",
    "\n",
    "# Count occurrences for initial probabilities\n",
    "for sent in train_data:\n",
    "    initial_tag_idx = tag_to_idx[sent[0][1]]\n",
    "    initial_probs[initial_tag_idx] += 1\n",
    "\n",
    "# Add-one (Laplace) smoothing and normalization\n",
    "transition_probs += 1\n",
    "emission_probs += 1\n",
    "initial_probs += 1\n",
    "\n",
    "transition_probs /= transition_probs.sum(axis=1, keepdims=True)\n",
    "emission_probs /= emission_probs.sum(axis=1, keepdims=True)\n",
    "initial_probs /= initial_probs.sum()\n",
    "\n",
    "# 3. Decoding (Inference) - Viterbi Algorithm\n",
    "\n",
    "def viterbi(obs, transition_probs, emission_probs, initial_probs):\n",
    "    num_states = transition_probs.shape[0]\n",
    "    num_obs = len(obs)\n",
    "\n",
    "    # Initialize the Viterbi matrix and backpointer matrix\n",
    "    viterbi_matrix = np.zeros((num_states, num_obs))\n",
    "    backpointer = np.zeros((num_states, num_obs), dtype=int)\n",
    "\n",
    "    # Initialization step\n",
    "    first_word_idx = word_to_idx.get(obs[0].lower(), 0)  # Handle unknown words\n",
    "    viterbi_matrix[:, 0] = initial_probs * emission_probs[:, first_word_idx]\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, num_obs):\n",
    "        word_idx = word_to_idx.get(obs[t].lower(), 0) # Handle unknown words\n",
    "        for s in range(num_states):\n",
    "            viterbi_scores = viterbi_matrix[:, t-1] * transition_probs[:, s] * emission_probs[s, word_idx]\n",
    "            best_prev_state = np.argmax(viterbi_scores)\n",
    "            viterbi_matrix[s, t] = viterbi_scores[best_prev_state]\n",
    "            backpointer[s, t] = best_prev_state\n",
    "\n",
    "    # Termination step\n",
    "    best_final_state = np.argmax(viterbi_matrix[:, num_obs - 1])\n",
    "    best_path = [best_final_state]\n",
    "\n",
    "    # Backtrack to find the best path\n",
    "    for t in range(num_obs - 1, 0, -1):\n",
    "        best_final_state = backpointer[best_final_state, t]\n",
    "        best_path.insert(0, best_final_state)\n",
    "\n",
    "    return [idx_to_tag[state_idx] for state_idx in best_path]\n",
    "\n",
    "# Example usage on a test sentence\n",
    "test_sentence = \"the quick brown fox jumps over the lazy dog.\".split()\n",
    "predicted_tags = viterbi(test_sentence, transition_probs, emission_probs, initial_probs)\n",
    "# predicted_tags = viterbi(test_sentence,states, transition_probs, emission_probs, initial_probs)\n",
    "\n",
    "# obs_seq, states, start_prob, trans_mat, emit_mat\n",
    "print(f\"Test Sentence: {test_sentence}\")\n",
    "print(f\"Predicted Tags: {predicted_tags}\")\n",
    "\n",
    "# Evaluation (very basic - just accuracy on the test set)\n",
    "correct = 0\n",
    "total = 0\n",
    "for sent in test_data:\n",
    "    words = [word.lower() for word, _ in sent]\n",
    "    actual_tags = [tag for _, tag in sent]\n",
    "    predicted_tags = viterbi(words, transition_probs, emission_probs, initial_probs)\n",
    "    # predicted_tags = viterbi(words,states, transition_probs, emission_probs, initial_probs)\n",
    "\n",
    "    for p, a in zip(predicted_tags, actual_tags):\n",
    "        if p == a:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc22393-e750-4849-9ae7-77ab51c38e6a",
   "metadata": {},
   "source": [
    "Code for Forward algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8cf54dd2-f512-4c16-9b09-c9fa1201097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(observations, trans_prob, emit_prob, initial_probs):\n",
    "    n_states = trans_prob.shape[0]\n",
    "    print('n_state is ', n_states)\n",
    "    n_obs = len(observations)\n",
    "    forward = np.zeros((n_states, n_obs))  # Matrix to store forward probabilities\n",
    "\n",
    "\n",
    "    # Initialization step\n",
    "    first_word_idx = word_to_idx.get(observations[0].lower(), 0)\n",
    "    print('first word idx is ', first_word_idx)\n",
    "    for state in range(n_states):\n",
    "        forward[state, 0] = (\n",
    "          initial_probs[state] * emit_prob[state, first_word_idx]\n",
    "        )\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, n_obs):\n",
    "        word_idx = word_to_idx.get(observations[t].lower(), 0)\n",
    "        for state in range(n_states):\n",
    "            forward[state, t] = sum(\n",
    "                forward[prev_state, t - 1]\n",
    "                * trans_prob[prev_state, state]\n",
    "                * emit_prob[state, word_idx]\n",
    "                for prev_state in range(n_states)\n",
    "            )\n",
    "\n",
    "    probability_of_observation = sum(forward[i, n_obs-1] for i in range(n_states))\n",
    "\n",
    "    return probability_of_observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabeae27-3072-4600-b65e-fbf2c0e42eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ccdc2-48cd-449e-9a6d-abc42b844fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07581e-682c-47da-b738-5d1649f46ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68546e1-baed-4a43-adda-90d5c600c24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201f143-3d77-47f0-bc47-b002f475be58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9941f75d-4b9f-4e5c-9808-8db58c0b11cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
